{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":127477,"databundleVersionId":15254714,"sourceType":"competition"}],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ðŸ–¥ï¸ Device: {device}\")\n\nprint(\"=\"*70)\nprint(\"ðŸ§  WEAK SIGNAL STRATEGY: PSEUDO-LABEL + PER-CLASS SELECTION\")\nprint(\"=\"*70)\n\n# ============================================================================\n# LOAD DATA\n# ============================================================================\nprint(\"\\n[1/7] Loading data...\")\ntrain = pd.read_csv('//kaggle/input/thapar-ucs-654-2026-hack-01/train.csv')\ntest = pd.read_csv('/kaggle/input/thapar-ucs-654-2026-hack-01/test.csv')\n\nTARGET_COL = 'target'\nID_COL = 'id'\nfeature_cols = [col for col in train.columns if col not in [TARGET_COL, ID_COL]]\n\ny = train[TARGET_COL].values\nnum_classes = 20\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Remove duplicates\ncorr_matrix = train[feature_cols].corr()\nfeatures = corr_matrix.columns.tolist()\nvisited = set()\nto_keep = []\nfor feat in features:\n    if feat not in visited:\n        to_keep.append(feat)\n        for other in features:\n            if other not in visited and abs(corr_matrix.loc[feat, other]) >= 0.999:\n                visited.add(other)\n        visited.add(feat)\n\nX_raw = train[to_keep].values\nX_test_raw = test[to_keep].values\n\nprint(f\"   Train: {X_raw.shape}, Test: {X_test_raw.shape}\")\n\n# Feature importance for polynomial\nrf_temp = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nrf_temp.fit(X_raw, y)\nimportance_order = np.argsort(rf_temp.feature_importances_)\n\n# ============================================================================\n# CREATE MAIN FEATURES\n# ============================================================================\nprint(\"\\n[2/7] Creating features...\")\n\ndef make_poly(X, X_test, top_k, interaction_only=True):\n    X_top = X[:, top_k]\n    X_test_top = X_test[:, top_k]\n    poly = PolynomialFeatures(degree=2, interaction_only=interaction_only, include_bias=False)\n    X_poly = poly.fit_transform(X_top)\n    X_test_poly = poly.transform(X_test_top)\n    X_full = np.hstack([X, X_poly])\n    X_test_full = np.hstack([X_test, X_test_poly])\n    scaler = StandardScaler()\n    return scaler.fit_transform(X_full), scaler.transform(X_test_full)\n\n# Main feature set\ntop_30 = importance_order[-30:]\nX_main, X_test_main = make_poly(X_raw, X_test_raw, top_30)\nprint(f\"   Main features: {X_main.shape[1]}\")\n\n# Alternative feature sets for diversity\ntop_25 = importance_order[-25:]\nX_v2, X_test_v2 = make_poly(X_raw, X_test_raw, top_25)\n\ntop_35 = importance_order[-35:]\nX_v3, X_test_v3 = make_poly(X_raw, X_test_raw, top_35)\n\n# ============================================================================\n# TRAIN BASE MODELS (Round 1)\n# ============================================================================\nprint(\"\\n[3/7] Training base models (Round 1)...\")\n\nall_oof = {}\nall_test = {}\n\n# --- SVM on different views ---\nviews = {\n    'svm_v1': (X_main, X_test_main),\n    'svm_v2': (X_v2, X_test_v2),\n    'svm_v3': (X_v3, X_test_v3),\n}\n\nfor name, (X_view, X_test_view) in views.items():\n    print(f\"\\n   Training {name}...\")\n    oof = np.zeros((len(y), num_classes))\n    test_pred = np.zeros((len(X_test_view), num_classes))\n    \n    for fold, (tr_idx, val_idx) in enumerate(cv.split(X_view, y)):\n        svm = SVC(kernel='rbf', C=5, gamma='scale', probability=True, random_state=42)\n        svm.fit(X_view[tr_idx], y[tr_idx])\n        oof[val_idx] = svm.predict_proba(X_view[val_idx])\n        test_pred += svm.predict_proba(X_test_view) / 5\n    \n    acc = accuracy_score(y, np.argmax(oof, axis=1))\n    all_oof[name] = oof\n    all_test[name] = test_pred\n    print(f\"   âœ… {name}: {acc*100:.2f}%\")\n\n# --- Gated Network ---\nprint(f\"\\n   Training Gated Network...\")\n\nclass GatedBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.net = nn.Sequential(nn.Linear(dim, dim*2), nn.GELU(), nn.Dropout(0.3), nn.Linear(dim*2, dim))\n        self.gate = nn.Sequential(nn.Linear(dim, dim), nn.Sigmoid())\n        self.norm = nn.LayerNorm(dim)\n    def forward(self, x):\n        return self.norm(x + self.net(x) * self.gate(x))\n\nclass GatedNet(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.gate = nn.Sequential(nn.Linear(input_dim, input_dim), nn.Sigmoid())\n        self.embed = nn.Sequential(nn.Linear(input_dim, 256), nn.LayerNorm(256), nn.GELU(), nn.Dropout(0.3))\n        self.b1, self.b2, self.b3 = GatedBlock(256), GatedBlock(256), GatedBlock(256)\n        self.out = nn.Sequential(nn.Linear(256, 128), nn.GELU(), nn.Dropout(0.15), nn.Linear(128, num_classes))\n    def forward(self, x):\n        x = x * self.gate(x)\n        x = self.b3(self.b2(self.b1(self.embed(x))))\n        return self.out(x)\n\ndef train_nn(model_cls, X_tr, y_tr, X_val, y_val, epochs=150, patience=20):\n    loader = DataLoader(TensorDataset(torch.FloatTensor(X_tr), torch.LongTensor(y_tr)), batch_size=256, shuffle=True)\n    model = model_cls(X_tr.shape[1], num_classes).to(device)\n    opt = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.02)\n    sched = optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, T_0=20, T_mult=2)\n    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n    best_acc, best_state, wait = 0, None, 0\n    for _ in range(epochs):\n        model.train()\n        for xb, yb in loader:\n            xb, yb = xb.to(device), yb.to(device)\n            opt.zero_grad()\n            crit(model(xb), yb).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            opt.step()\n        sched.step()\n        model.eval()\n        with torch.no_grad():\n            pred = torch.argmax(model(torch.FloatTensor(X_val).to(device)), dim=1).cpu().numpy()\n        acc = accuracy_score(y_val, pred)\n        if acc > best_acc:\n            best_acc, best_state, wait = acc, model.state_dict().copy(), 0\n        else:\n            wait += 1\n            if wait >= patience: break\n    model.load_state_dict(best_state)\n    return model\n\noof = np.zeros((len(y), num_classes))\ntest_pred = np.zeros((len(X_test_main), num_classes))\nfor fold, (tr_idx, val_idx) in enumerate(cv.split(X_main, y)):\n    model = train_nn(GatedNet, X_main[tr_idx], y[tr_idx], X_main[val_idx], y[val_idx])\n    model.eval()\n    with torch.no_grad():\n        oof[val_idx] = torch.softmax(model(torch.FloatTensor(X_main[val_idx]).to(device)), dim=1).cpu().numpy()\n        test_pred += torch.softmax(model(torch.FloatTensor(X_test_main).to(device)), dim=1).cpu().numpy() / 5\n\nacc = accuracy_score(y, np.argmax(oof, axis=1))\nall_oof['gated'] = oof\nall_test['gated'] = test_pred\nprint(f\"   âœ… gated: {acc*100:.2f}%\")\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n# ============================================================================\n# PSEUDO-LABELING\n# ============================================================================\nprint(\"\\n[4/7] PSEUDO-LABELING (using test data!)...\")\n\n# Average predictions from all models\navg_test_proba = np.mean([all_test[n] for n in all_oof.keys()], axis=0)\navg_test_pred = np.argmax(avg_test_proba, axis=1)\navg_test_conf = np.max(avg_test_proba, axis=1)\n\n# Select HIGH CONFIDENCE predictions (top 30%)\nconfidence_threshold = np.percentile(avg_test_conf, 70)\nhigh_conf_mask = avg_test_conf >= confidence_threshold\nn_pseudo = high_conf_mask.sum()\n\nprint(f\"   Confidence threshold: {confidence_threshold:.3f}\")\nprint(f\"   High confidence samples: {n_pseudo} ({n_pseudo/len(avg_test_conf)*100:.1f}%)\")\n\n# Create pseudo-labeled dataset\nX_pseudo = X_test_main[high_conf_mask]\ny_pseudo = avg_test_pred[high_conf_mask]\n\n# Combine with original training data\nX_combined = np.vstack([X_main, X_pseudo])\ny_combined = np.hstack([y, y_pseudo])\n\nprint(f\"   Original train: {len(y)}, After pseudo-label: {len(y_combined)}\")\n\n# ============================================================================\n# TRAIN WITH PSEUDO-LABELS (Round 2)\n# ============================================================================\nprint(\"\\n[5/7] Training with pseudo-labels (Round 2)...\")\n\n# Train SVM on combined data\nprint(\"\\n   Training SVM with pseudo-labels...\")\noof_pseudo = np.zeros((len(y), num_classes))\ntest_pseudo = np.zeros((len(X_test_main), num_classes))\n\nfor fold, (tr_idx, val_idx) in enumerate(cv.split(X_main, y)):\n    # Use original train indices but train on combined data concept\n    # Actually, let's train on full combined and validate on original OOF\n    svm = SVC(kernel='rbf', C=5, gamma='scale', probability=True, random_state=42)\n    \n    # For this fold, add pseudo-labels to training\n    X_tr_fold = np.vstack([X_main[tr_idx], X_pseudo])\n    y_tr_fold = np.hstack([y[tr_idx], y_pseudo])\n    \n    svm.fit(X_tr_fold, y_tr_fold)\n    oof_pseudo[val_idx] = svm.predict_proba(X_main[val_idx])\n    test_pseudo += svm.predict_proba(X_test_main) / 5\n\nacc_pseudo = accuracy_score(y, np.argmax(oof_pseudo, axis=1))\nall_oof['svm_pseudo'] = oof_pseudo\nall_test['svm_pseudo'] = test_pseudo\nprint(f\"   âœ… SVM + Pseudo-labels: {acc_pseudo*100:.2f}%\")\n\n# Train Gated with pseudo-labels\nprint(\"\\n   Training Gated with pseudo-labels...\")\noof_gated_pseudo = np.zeros((len(y), num_classes))\ntest_gated_pseudo = np.zeros((len(X_test_main), num_classes))\n\nfor fold, (tr_idx, val_idx) in enumerate(cv.split(X_main, y)):\n    X_tr_fold = np.vstack([X_main[tr_idx], X_pseudo])\n    y_tr_fold = np.hstack([y[tr_idx], y_pseudo])\n    \n    model = train_nn(GatedNet, X_tr_fold, y_tr_fold, X_main[val_idx], y[val_idx], epochs=100, patience=15)\n    model.eval()\n    with torch.no_grad():\n        oof_gated_pseudo[val_idx] = torch.softmax(model(torch.FloatTensor(X_main[val_idx]).to(device)), dim=1).cpu().numpy()\n        test_gated_pseudo += torch.softmax(model(torch.FloatTensor(X_test_main).to(device)), dim=1).cpu().numpy() / 5\n\nacc_gated_pseudo = accuracy_score(y, np.argmax(oof_gated_pseudo, axis=1))\nall_oof['gated_pseudo'] = oof_gated_pseudo\nall_test['gated_pseudo'] = test_gated_pseudo\nprint(f\"   âœ… Gated + Pseudo-labels: {acc_gated_pseudo*100:.2f}%\")\n\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n# ============================================================================\n# PER-CLASS MODEL SELECTION (NOT STACKING!)\n# ============================================================================\nprint(\"\\n[6/7] PER-CLASS MODEL SELECTION...\")\n\nmodel_names = list(all_oof.keys())\nprint(f\"\\n   Models available: {model_names}\")\n\n# For each class, find which model has highest recall\nper_class_best = {}\nfor c in range(num_classes):\n    best_model = None\n    best_recall = 0\n    \n    for name in model_names:\n        # Get predictions for this class\n        pred = np.argmax(all_oof[name], axis=1)\n        true_mask = (y == c)\n        if true_mask.sum() == 0:\n            continue\n        \n        # Recall for this class\n        recall = (pred[true_mask] == c).sum() / true_mask.sum()\n        \n        if recall > best_recall:\n            best_recall = recall\n            best_model = name\n    \n    per_class_best[c] = best_model\n    print(f\"   Class {c:2d}: {best_model} (recall={best_recall:.2f})\")\n\n# Create per-class ensemble prediction\ndef per_class_predict(oof_dict, test_dict, per_class_best, num_classes):\n    \"\"\"Select best model's probability for each class\"\"\"\n    n_train = len(list(oof_dict.values())[0])\n    n_test = len(list(test_dict.values())[0])\n    \n    oof_combined = np.zeros((n_train, num_classes))\n    test_combined = np.zeros((n_test, num_classes))\n    \n    for c in range(num_classes):\n        best_model = per_class_best[c]\n        oof_combined[:, c] = oof_dict[best_model][:, c]\n        test_combined[:, c] = test_dict[best_model][:, c]\n    \n    # Normalize to sum to 1\n    oof_combined = oof_combined / (oof_combined.sum(axis=1, keepdims=True) + 1e-10)\n    test_combined = test_combined / (test_combined.sum(axis=1, keepdims=True) + 1e-10)\n    \n    return oof_combined, test_combined\n\noof_perclass, test_perclass = per_class_predict(all_oof, all_test, per_class_best, num_classes)\nacc_perclass = accuracy_score(y, np.argmax(oof_perclass, axis=1))\nprint(f\"\\n   âœ… Per-Class Selection: {acc_perclass*100:.2f}%\")\n\n# ============================================================================\n# FINAL COMBINATIONS\n# ============================================================================\nprint(\"\\n[7/7] Final combinations...\")\n\n# Simple average of all\nsimple_avg = np.mean([all_oof[n] for n in model_names], axis=0)\nsimple_acc = accuracy_score(y, np.argmax(simple_avg, axis=1))\nprint(f\"\\n   Simple Average (all): {simple_acc*100:.2f}%\")\n\n# Average of pseudo-labeled models only\npseudo_models = ['svm_pseudo', 'gated_pseudo']\npseudo_avg = np.mean([all_oof[n] for n in pseudo_models], axis=0)\npseudo_avg_acc = accuracy_score(y, np.argmax(pseudo_avg, axis=1))\nprint(f\"   Pseudo-label Average: {pseudo_avg_acc*100:.2f}%\")\n\n# Combine per-class with pseudo average\ncombined = 0.5 * oof_perclass + 0.5 * pseudo_avg\ncombined_acc = accuracy_score(y, np.argmax(combined, axis=1))\nprint(f\"   PerClass + Pseudo: {combined_acc*100:.2f}%\")\n\n# Try different blends\nprint(\"\\n   Searching blends...\")\nbest_blend_acc = 0\nbest_blend = None\nbest_blend_test = None\n\nfor w1 in np.arange(0, 1.05, 0.1):\n    for w2 in np.arange(0, 1.05 - w1, 0.1):\n        w3 = 1 - w1 - w2\n        if w3 < 0: continue\n        \n        blend = w1 * simple_avg + w2 * oof_perclass + w3 * pseudo_avg\n        acc = accuracy_score(y, np.argmax(blend, axis=1))\n        \n        if acc > best_blend_acc:\n            best_blend_acc = acc\n            best_blend = blend\n            best_blend_test = (w1 * np.mean([all_test[n] for n in model_names], axis=0) + \n                              w2 * test_perclass + \n                              w3 * np.mean([all_test[n] for n in pseudo_models], axis=0))\n\nprint(f\"   Best Blend: {best_blend_acc*100:.2f}%\")\n\n# ============================================================================\n# RESULTS\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ“Š FINAL RESULTS\")\nprint(\"=\"*70)\n\nresults = {\n    'Simple Average': simple_acc,\n    'Per-Class Selection': acc_perclass,\n    'Pseudo-label Average': pseudo_avg_acc,\n    'PerClass + Pseudo': combined_acc,\n    'Best Blend': best_blend_acc,\n}\n\n# Individual models\nprint(\"\\n   Individual Models:\")\nfor name in model_names:\n    acc = accuracy_score(y, np.argmax(all_oof[name], axis=1))\n    print(f\"      {name}: {acc*100:.2f}%\")\n\nprint(\"\\n   Combination Methods:\")\nfor name, acc in sorted(results.items(), key=lambda x: -x[1]):\n    marker = \"ðŸ†\" if acc == max(results.values()) else \"  \"\n    print(f\"   {marker} {name}: {acc*100:.2f}%\")\n\n# Pick best\nbest_method = max(results, key=lambda x: results[x])\nbest_acc = results[best_method]\n\n# ============================================================================\n# SUBMISSION\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸ“ GENERATING SUBMISSION\")\nprint(\"=\"*70)\n\n# Use best method\nif best_method == 'Best Blend':\n    final_test = best_blend_test\nelif best_method == 'Per-Class Selection':\n    final_test = test_perclass\nelif best_method == 'Pseudo-label Average':\n    final_test = np.mean([all_test[n] for n in pseudo_models], axis=0)\nelif best_method == 'PerClass + Pseudo':\n    final_test = 0.5 * test_perclass + 0.5 * np.mean([all_test[n] for n in pseudo_models], axis=0)\nelse:\n    final_test = np.mean([all_test[n] for n in model_names], axis=0)\n\nfinal_pred = np.argmax(final_test, axis=1).astype(int)\n\nsubmission = pd.DataFrame({\n    'id': test['id'].values,\n    'target': final_pred\n})\n\nsubmission.to_csv('/kaggle/working/submission_pseudo_perclass.csv', index=False)\nprint(f\"\\nâœ… Saved: submission_pseudo_perclass.csv\")\nprint(f\"   Method: {best_method}\")\nprint(f\"   OOF Accuracy: {best_acc*100:.2f}%\")\n\nprint(\"\\n\" + \"=\"*70)\nif best_acc >= 0.28:\n    print(f\"âœ… TARGET: {best_acc*100:.2f}% >= 28%\")\nelse:\n    print(f\"ðŸ“ˆ Best: {best_acc*100:.2f}%\")\nprint(\"=\"*70)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-11T12:52:38.759519Z","iopub.execute_input":"2026-01-11T12:52:38.759797Z","iopub.status.idle":"2026-01-11T14:11:34.088548Z","shell.execute_reply.started":"2026-01-11T12:52:38.759766Z","shell.execute_reply":"2026-01-11T14:11:34.087695Z"}},"outputs":[{"name":"stdout","text":"ðŸ–¥ï¸ Device: cuda\n======================================================================\nðŸ§  WEAK SIGNAL STRATEGY: PSEUDO-LABEL + PER-CLASS SELECTION\n======================================================================\n\n[1/7] Loading data...\n   Train: (10000, 50), Test: (10000, 50)\n\n[2/7] Creating features...\n   Main features: 515\n\n[3/7] Training base models (Round 1)...\n\n   Training svm_v1...\n   âœ… svm_v1: 22.15%\n\n   Training svm_v2...\n   âœ… svm_v2: 22.63%\n\n   Training svm_v3...\n   âœ… svm_v3: 18.41%\n\n   Training Gated Network...\n   âœ… gated: 21.45%\n\n[4/7] PSEUDO-LABELING (using test data!)...\n   Confidence threshold: 0.229\n   High confidence samples: 3000 (30.0%)\n   Original train: 10000, After pseudo-label: 13000\n\n[5/7] Training with pseudo-labels (Round 2)...\n\n   Training SVM with pseudo-labels...\n   âœ… SVM + Pseudo-labels: 26.98%\n\n   Training Gated with pseudo-labels...\n   âœ… Gated + Pseudo-labels: 27.41%\n\n[6/7] PER-CLASS MODEL SELECTION...\n\n   Models available: ['svm_v1', 'svm_v2', 'svm_v3', 'gated', 'svm_pseudo', 'gated_pseudo']\n   Class  0: gated (recall=0.23)\n   Class  1: svm_pseudo (recall=0.33)\n   Class  2: gated_pseudo (recall=0.32)\n   Class  3: svm_pseudo (recall=0.35)\n   Class  4: gated_pseudo (recall=0.29)\n   Class  5: svm_pseudo (recall=0.27)\n   Class  6: gated_pseudo (recall=0.24)\n   Class  7: gated_pseudo (recall=0.32)\n   Class  8: gated_pseudo (recall=0.26)\n   Class  9: gated_pseudo (recall=0.30)\n   Class 10: svm_pseudo (recall=0.26)\n   Class 11: gated_pseudo (recall=0.23)\n   Class 12: gated_pseudo (recall=0.26)\n   Class 13: gated_pseudo (recall=0.26)\n   Class 14: gated_pseudo (recall=0.29)\n   Class 15: svm_pseudo (recall=0.27)\n   Class 16: gated_pseudo (recall=0.31)\n   Class 17: gated_pseudo (recall=0.32)\n   Class 18: svm_pseudo (recall=0.28)\n   Class 19: svm_pseudo (recall=0.23)\n\n   âœ… Per-Class Selection: 27.22%\n\n[7/7] Final combinations...\n\n   Simple Average (all): 27.06%\n   Pseudo-label Average: 27.95%\n   PerClass + Pseudo: 27.83%\n\n   Searching blends...\n   Best Blend: 28.22%\n\n======================================================================\nðŸ“Š FINAL RESULTS\n======================================================================\n\n   Individual Models:\n      svm_v1: 22.15%\n      svm_v2: 22.63%\n      svm_v3: 18.41%\n      gated: 21.45%\n      svm_pseudo: 26.98%\n      gated_pseudo: 27.41%\n\n   Combination Methods:\n   ðŸ† Best Blend: 28.22%\n      Pseudo-label Average: 27.95%\n      PerClass + Pseudo: 27.83%\n      Per-Class Selection: 27.22%\n      Simple Average: 27.06%\n\n======================================================================\nðŸ“ GENERATING SUBMISSION\n======================================================================\n\nâœ… Saved: submission_pseudo_perclass.csv\n   Method: Best Blend\n   OOF Accuracy: 28.22%\n\n======================================================================\nâœ… TARGET: 28.22% >= 28%\n======================================================================\n","output_type":"stream"}],"execution_count":1}]}